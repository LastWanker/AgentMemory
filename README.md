计划书：自监督“记忆唤醒索引器”（文本 → 索引场/向量组 → 检索记忆）
1. 目标与边界：我们到底在做什么、不是在做什么

你要做的东西，本质上不是“让机器记住文本”，也不是“让机器读心”，而是：

输入：当前一段文本（比如对话的开头/当前轮）
输出：一种“检索指令”——不是直接给出记忆ID，而是给出一个可用于检索的索引场 / 向量组，让系统能从记忆库里找出“可能应该被唤醒”的记忆片段（Top-k）。

关键边界（必须写死，否则会无限发散）：

我们不追求“还原人脑想了什么”。我们追求“对后续文本有用的唤醒”。

监督信号不依赖人工标注（允许少量标注做加速/验证，但不是前提）。

你个人设备算力有限，所以不训练大模型；最多训练很小的头（小网络）或只做无训练/弱训练的工程方案。

2. 术语小词典（你不需要会公式，但要知道每个符号在干嘛）

为了写计划书不啰嗦，我会用一些符号，但每个符号都只有一个意思：

x：当前输入文本（比如 A：“坐不住了…”）

y：未来文本（后续一句/一段，比如 B：“我没带火…” 或更长的后续）

Encoder（编码器）：把文本变成向量的模型（可以是 BERT、E5、bge、text2vec 等）

Q（Query 向量组 / 索引场）：由 x 生成的“检索指令”。你可以把它当成“我现在在找什么”的几个方向。

M（Memory 向量组）：每条记忆自身的向量组。你可以把它当成“这条记忆包含哪些可被触发的面”。

K（Key 向量组）：由 y 生成的表示，训练时拿它当“目标方向”。你可以把它理解为“未来真正走向的线索集合”。（实现上 K 很多时候和 M 是同一种编码方式，只是叫法不同）

Top-k：从候选里挑分数最高的 k 条/块。

Adapter / LoRA：两种“只训练很少参数”的微调方式（不用你从零训练模型）。你现在可以先当它们不存在；后面需要个性化时再用。

直白讲：底座模型不动，只在旁边加一小撮可训练“插件”，学个人偏好。

3. 整体系统长什么样（你要的“全貌”）

系统分三块：记忆库构建 → 检索 →（可选）自监督改进。

3.1 记忆库构建（把文本变成“可检索记忆”）

你会有一堆“记忆条目”，来源可以是：

历史对话（你和朋友、你和AI、你自己的日志）

网络对话语料（公开论坛、对话数据集）

你自己写的“事件摘要”（少量即可）

每条记忆不再只存一个向量，而是存一个向量组（M）。
这一步的目的就是你说的：让记忆携带更多信息量，不被“平均向量”抹平。

3.2 检索（输入一句话，唤醒相关记忆）

给定输入 x：

把 x 编码成向量组 Q（索引场）

用某种“向量组相似度”给记忆库里每条 M 打分（实际上不会对全库逐条算，会有粗召回）

输出 top-k 记忆（或它们的ID/索引）

3.3 可选：自监督改进（用后文 y 来提升 Q 的质量）

这块是“锦上添花”，MVP 可以先不做或做最轻版本。

训练样本来自时间结构：

取对话中的一段作为 x

取后续一段作为 y

让模型学：Q(x) 应该更贴近 K(y)，并远离其他乱七八糟的 y’（负样本）

你不需要人工写“正确答案是哪个记忆”，后文 y 就是监督信号。

4. MVP（最小可行产品）：不训练大模型也能跑起来的版本

MVP 的目标不是完美联想，而是：

有一个可用的记忆库

输入一句话能检索到“看起来像样的”几条记忆

能跑、可调参、能观察失败案例

4.1 选择现成编码器（最关键的第一步）

你要一个“把中文文本变成向量”的模型。你不训练它，只使用它。

建议路线（按省事程度）：

句向量模型（直接给你一段话一个向量）：更省事，但单向量信息容易糊

Token 级模型（BERT 输出每个 token 一个向量）：更符合你的“向量组/场”想法

MVP 我建议这样折中：

记忆条目用 token 向量组（信息更足）

粗召回用 句向量（速度更快）

4.2 记忆条目怎么切（记忆不是越大越好）

你需要一个“记忆粒度”。MVP 先简单：

每个对话 turn、或连续 1～3 句，作为一个记忆条目

太长会混主题，太短又没信息

先用固定窗口就行，后面再学智能切块。

4.3 如何从一条记忆生成“向量组 M”

你不想平均，那就别平均。MVP 用可解释的“挑 token”法：

对记忆文本跑 BERT，得到每个 token 的向量（一串向量）。然后挑出其中一部分作为 M。

最朴素、最容易实现的挑法（不需要训练）：

去掉停用词/标点/过短词

只保留名词/动词/形容词（可选：用一个简易分词+词性工具）

取 TF-IDF 高的词对应位置

或取“自注意力较高”的 token（如果你愿意看模型注意力）

MVP 建议：TF-IDF + 停用词过滤够用了。
每条记忆保留 16～64 个 token 向量就行（先固定 32）。

这样每条记忆就从“一个点”变成了“一个小点云”。

4.4 Query（输入 x）怎么生成“向量组 Q”

同理，对输入 x：

也跑一次编码器，得到 token 向量

也挑 16～64 个，形成 Q

MVP 里 Q 和 M 的生成规则可以完全一样（降低变量）。

4.5 向量组相似度怎么打分（核心评分函数）

你现在只要记住一句话：
“只要某一面强对齐，就算匹配成功。”

实现成打分就是：

对 Q 中每个向量 q，找 M 中最相近的向量 m（最大余弦相似度）

你得到一堆“每个 q 的最好匹配分”

把这些分取 top-3 平均，或者直接平均，作为这条记忆的最终分数

直觉：x 的某些关键 token，只要能在记忆里找到很强的对应，就会把这条记忆抬上去。

4.6 粗召回：不用全库逐条算

你有上万条记忆，逐条算“向量组相似度”会慢。

MVP 的粗召回可以粗暴但有效：

每条记忆再额外存一个“句向量”（哪怕是 token 向量平均也行，用于粗召回）

对输入 x 也算一个句向量

用句向量做 ANN（近似最近邻）或先做简单余弦排序，先捞 top-500 或 top-1000

再对这批候选做精排（向量组相似度）

“会不会误伤无辜？”——粗召回宁可多捞。你的精排会救回来。MVP 目标是“不漏得太离谱”。

4.7 MVP 的交付物

你要把成果做成“可观察”的工具，而不是一坨黑箱：

一个脚本/模块：build_index（构建记忆库向量）

一个脚本/模块：retrieve（输入一句话 → 输出 top-k 记忆 + 分数 + 命中的 token）

一个小测试集：20～50 组对话片段，用来肉眼检查

MVP 的成功标准很朴素：

输入“充电/坐不住/出去”能检到“抽烟/借火/朋友顺火机”这类条目比乱七八糟的更靠前

失败案例能解释：到底是分词错、停用词错、还是记忆切块混了主题

5. 自监督升级版：真正利用后文 y（不靠人工标注也能学）

当 MVP 能跑，你想让它更“会联想”，就引入最轻的自监督。注意：这一步也可以只训练一个小头，底座不动。

5.1 数据怎么来（材料从哪找）

你需要大量 (x, y) 对：

网络公开对话数据集（中文闲聊、论坛楼层）

你自己历史聊天记录（最个性化）

你写的“事件—后续”片段（少量补充）

构造规则很简单：

从对话中随机截一个位置

x = 前一段（比如 1 句或 1 turn）

y = 后一段（比如紧接着的 1 句或 1～2 turn）

这就是“后文监督”。

5.2 训练目标到底是什么（别让它回到“像不像”）

你担心“想了什么和说了什么不同”，所以我们不直接监督“想到了哪条记忆”，而监督更弱但可行的东西：

让 Q(x) 更容易找回 K(y)
这里 K(y) 就是 y 的向量组（同编码器同挑选规则）。

直觉：如果 x 的索引场能对齐到未来 y 的关键 token，那它就学到了“会往哪类线索上找”。

5.3 负样本怎么来（你最关心的坑）

你不需要手工造“低质量联想”。

最省事的负样本来源是“同一批次的其他 y”：

你拿一批 (x_i, y_i)

对 x_i 来说，y_i 是正例

其他 y_j（j≠i）天然是负例
这就是常用的 in-batch negatives

想要硬负样本（“很像但不该”）也很容易：

用粗召回给 x 找一堆“看起来像的 y 候选”

除了真实 y 之外，其余都可当硬负样本
这比让 LLM 编生活更生活。

5.4 防坍缩（你说“我觉得不可能”的那个）

坍缩不是随机性问题，是“目标太松导致常量解”。

如果你的训练只要求“平均意义上更接近”，模型可能学会输出一个“万能 Q”，什么都差不多。
对比学习（必须把正例拉近、负例推远）能自然避免常量解，因为常量 Q 不可能同时区分很多正负对。

你不需要深奥理论，只要记住：
训练里必须有“推远”这一半，光“拉近”会塌。

5.5 你没有算力怎么办（只训练小东西）

可行路径有两种：

路径A：完全不训练底座，只训练一个小“投影头”（MLP）
作用：把 BERT 的 token 向量做一次小变换，让它更适配“未来对齐”

路径B：用 Adapter/LoRA 做极少参数微调
你只训练几百万甚至几十万参数，而不是几亿

你可以先走路径A，简单到离谱，也足够你看到提升。

6. 逐步优化路线图（从能用到好用）

当你有了 MVP + 轻自监督，你的升级基本沿着这几条走：

6.1 更聪明的“挑 token”策略（提升向量组质量）

从 TF-IDF 升级到“实体/关键词抽取”

加入简单的事件结构：谁/做了什么/为什么（哪怕规则抽）

对记忆文本先做一遍“摘要/标题生成”（可以用现成小模型或规则），摘要也做向量组，双通道检索

6.2 多查询向量（让 Q 真像“场”）

现在 Q 是挑 token 得到的点云。你可以升级成“几个代表方向”：

对 Q 的 token 向量做聚类，取 3～8 个中心作为 query 向量

或用规则挑 3～8 个“最关键词”的 token 向量

这一步很符合你想象的“索引场”，而且不训练也能做。

6.3 更强的精排（只在 top-500 上跑）

如果你愿意加一点点计算：

对候选记忆，把 (x, 记忆文本) 拼起来喂一个小 reranker（交叉编码器）输出分数

只在粗召回候选上跑，所以可承受

这是提升“关系相关性”的大杀器，但不是 MVP 必需。

6.4 个性化（等你有自己的对话数据再做）

先不搞“每个人一个大模型”。
只做：

记忆库就是个性化

训练一个小投影头就是个性化

未来再考虑 Adapter/LoRA

7. 你可能遇到的困难（提前写在计划书里，省得你崩溃）

你会踩的坑，基本集中在这些地方：

(1) 记忆切块不合理
太长：混主题，检索糊
太短：信息不足，匹配靠运气
解决：先固定窗口，后面做更智能切块

(2) 中文分词/停用词处理影响巨大
停用词没过滤干净，会让“的、了、啊”这种 token 污染向量组选取
解决：先上一个靠谱停用词表，必要时加入词性过滤

(3) 相似度函数太敏感/太迟钝
max-sim 可能过于“只要撞到一个词就上天”
mean-sim 可能又过于平均
解决：top-k 聚合（比如取 top-3 或 top-5 的均值），通常最稳

(4) 粗召回漏召回
这会让精排再强也救不了
解决：粗召回阈值调大（top-1000），宁可多算一点精排

(5) 自监督训练不稳定
如果你上来就想训得很聪明，容易陷入“感觉没提升”的挫败
解决：先做无训练版本把链路跑通，再做小投影头训练，逐步加料

8. 里程碑与时间顺序（你应该按这个顺序干，别反过来）

第1阶段：跑通检索闭环（完全不训练）

选一个中文编码器

记忆切块

生成记忆向量组 M

生成 query 向量组 Q

粗召回 + 精排

输出 top-k + 可解释日志（命中 token）

交付：你能拿几条例子反复试，看到它“像个东西”。

第2阶段：把“向量组/场”变得更像你脑子里的那个

改 token 选择策略（关键词/实体/聚类中心）

让 Q 变成 3～8 个中心向量

调整相似度聚合方式（top-k mean 等）

交付：你能稳定复现一些“不是纯语义相似、而是关联”的唤醒。

第3阶段：轻自监督（只训练小投影头）

用网络对话/历史记录构造 (x,y)

用 in-batch negatives 做对比学习

只训练一个小头，不动底座

交付：你能在一组固定测试上看到 top-k 的“相关性”有统计提升。

第4阶段：高级但可选

硬负样本挖掘（从粗召回候选里挖）

reranker 精排（只在小候选上）

个性化小插件（Adapter/LoRA）

9. “去哪找材料”的现实建议（不花里胡哨）

你需要两类材料：

(A) 对话数据（做 x→y）

公开中文对话数据集（闲聊、论坛楼层、QA）

你自己的聊天记录（最有价值）

(B) 记忆库内容（做 M）

你自己的历史对话/笔记/日志（哪怕很短）

少量人工编写“生活化记忆条目”（为了冷启动测试）

网络素材也可以，但要注意风格差异会影响检索质感

MVP 阶段别追求“数据大”，追求“能验证你的直觉”。

10. 你要的“完备计划”最后一句话（不煽情、不恭维）

你现在最该做的不是继续争论“理论完不完美”，而是把可观察闭环建起来：
先用现成模型把“向量组记忆 + 向量组查询 + 粗召回 + 精排”跑通；跑通之后，你就有了一个能承载改进的骨架，自监督训练、硬负样本、个性化插件，全都能在这个骨架上逐步加，不会变成空谈。

如果你愿意更“工程化”一点点，我下一步可以把 MVP 的接口设计也写出来：索引文件格式、每条记忆存哪些字段、检索日志输出哪些可解释信息、以及一套最小测试用例的组织方式。
