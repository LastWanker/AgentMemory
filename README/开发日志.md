【维护规则】本日志仅允许“追加补充”，禁止删除或改写既有内容。


# 开发日志（项目现状快照）
更新时间：2026-02-15

## 1. 项目定位与当前阶段
- 项目名称：Memory Indexer（记忆唤醒索引器）。
- 当前主目标：把输入文本转成“可检索指令”，通过粗召回 + 精排 + 路由输出 Top-k 记忆。
- 阶段判断：
  - MVP 检索闭环已成型（可建库、可检索、可评估、可演示）。
  - 软/半硬/硬路由接口已落地，可观测指标已接入。
  - 自监督训练仍是“骨架接口 + 小规模实验脚本”阶段，未形成稳定训练管线。

## 2. 代码结构现状
- 核心包：`src/memory_indexer/`
  - `pipeline.py`：端到端入口（建库 + 查询），含 coarse role 断言、缓存读写、检索委托。
  - `retriever.py`：候选集构建、精排打分、路由融合、指标输出（熵/覆盖/一致性/反事实等）。
  - `index.py`：粗召回索引 + 词法索引（支持 coarse / lexical / union 候选模式）。
  - `vectorizer.py`：向量组构造策略（当前主用 token_pool_topk）。
  - `encoder/`：基础编码接口 + simple/e5/hf sentence 编码器实现。
  - `scorer.py`：向量组打分器 + learned scorer（可加载 tiny reranker 权重）。
  - `ingest.py` / `models.py` / `store.py`：数据模型、入库、内存存储。
  - `trainer.py`：训练器占位，当前仅保留接口并返回基础报告。
- 脚本层：`scripts/`
  - `demo.py`：最小演示（默认 simple encoder，可切 learned scorer）。
  - `eval_router.py`：路由策略评测主脚本（Recall@k、MRR、top1、路由指标）。
  - `train_reranker.py` / `regression_learned_scorer.py`：轻量训练与回归验证。
- 数据层：`data/`
  - `Memory_Eval/`：easy/normal 两套 memory+eval 数据。
  - `VectorCache/`：query 与 memory 缓存（JSONL）。
  - `ModelWeights/`：`tiny_reranker.pt` 已存在，可直接用于 learned scorer。

## 3. 功能完成度
- 已完成：
  - 记忆库建库：文本 -> token 向量组 + coarse 向量 + 词法 token。
  - 缓存机制：按 encoder_id + strategy 过滤复用，降低重复编码成本。
  - 检索流程：coarse/lexical/union 候选 -> 向量组精排 -> 路由融合 -> Top-k 输出。
  - 路由策略：soft / half_hard / hard 三种 policy。
  - 路由监控：entropy、mass_at_k、consistency、counterfactual drop、candidate_size、coarse_lexical_jaccard。
  - 可解释输出：channel_weights、feature_keys、Top-ranked ids、候选模式说明。
- 部分完成：
  - learned scorer 已接入推理链路，但训练与评测规模仍偏小。
  - 多编码器组合（sentence + token）已支持，但默认 demo 仍以 simple encoder 为主。
- 未完成/待加强：
  - 真正可用的自监督训练器（当前 trainer 为占位实现）。
  - 系统级持久化索引（如 FAISS 真正落盘管理）与大规模性能压测。
  - 更稳定的中文语义编码默认方案（目前更偏工程骨架验证）。

## 4. 当前工程约束与已知风险
- 路由层风险：
  - soft 路由虽然可解释，但对“选错即失败”的约束仍弱。
  - hard 路由对召回质量更敏感，粗召回漏召回会直接放大错误。
- 数据风险：
  - 评测集规模仍小，容易高估策略效果。
  - easy/normal 集分布和真实线上数据存在差距。
- 表征风险：
  - simple encoder 便于回归，但语义上限低。
  - token 策略在中文场景仍受分词与停用词质量影响。

## 5. 可复现实验入口（给后续续写者）
- 建议先跑：
  1) `python scripts/demo.py`
  2) `python scripts/eval_router.py --dataset easy`
  3) `python scripts/eval_router.py --dataset normal --candidate-mode union`
- 若需验证 learned scorer：
  - 在上述命令加 `--use-learned-scorer --reranker-path data/ModelWeights/tiny_reranker.pt`。
- 观察重点：
  - 先看 Recall@k / MRR / top1，再看 entropy 与 mass_at_k 是否和策略预期一致。
  - 若 hard policy 指标异常，先排查 candidate_size 与 coarse_lexical_jaccard。

## 6. 下阶段优先级（执行建议）
- P0（优先马上做）
  - 固化一套“默认可复现评测命令 + 结果记录模板”（easy/normal 各一套）。
  - 补充小规模回归门槛（至少 top1、MRR、entropy 三指标阈值）。
- P1（短期）
  - 把 trainer 从占位升级为可训练小投影头，先接 in-batch negatives。
  - 扩展数据切分与缓存版本号，防止旧缓存污染新实验。
- P2（中期）
  - 加强 reranker 训练数据质量，做 hard negative 挖掘。
  - 增加“反事实删记忆”批量评估脚本，量化路由真实性。

## 7. 续写规范
- 仅在本文件末尾追加，不改历史条目。
- 每次追加必须包含：日期、变更摘要、关键函数名等等，其余自由发挥。
- 不必十分格式化，只需要政体简要、重要细节精准无误即可。

## 2026-02-15（learned scorer 分数饱和修复）

### 本次变更摘要
- 将 `LearnedFieldScorer.score()` 的推理主分从 `sigmoid(raw_score)` 改为 `raw_score`。
- debug 信息保留了三项：`raw_score`、`sigmoid_score`、`semantic_score`，方便对比。
- `scripts/eval_router.py` 的调试输出提升到 6 位小数，减少“显示上看不出差异”的假象。

### 经验教训
1. **不要把“训练常用激活”机械搬到“线上排序主分”**。
   - sigmoid 在高分区很容易饱和，分数看起来都差不多。
   - 一旦主分丢失差异，后面的路由和融合再精致也救不回来。

2. **先确认“信息有没有被你自己抹平”，再怀疑模型能力**。
   - 这次 raw 分其实有明显波动，但输出分被压平。
   - 排障顺序应该是：原始信号 -> 映射函数 -> 融合策略。
   - 下一步要做的是融合前的尺度校准（例如 z-score），不是回退修复。

### 后续行动建议
- 先在 easy / normal 数据集重跑对比，确认是否恢复“可分辨排序”。
- 增加一个简单守护检查：若 topN semantic 方差过低则报警。
- 规划 Router 融合前的通道标准化实验，避免某一路通道长期“独大”。

## 2026-02-15（多正例训练目标 + Bootstrap 评测 + 数量预测雏形）

### 本次变更摘要
- `scripts/train_reranker.py` 升级为兼容新旧数据格式：支持 `positives` 多正例字段，并对旧 `expected_mem_ids` 自动兼容。
- 训练新增 `--neg-strategy random|ranked`，可优先采样“排名靠前但非正例”的临界区负例。
- 训练新增 `--loss-type pairwise|listwise`：在 listwise 模式下直接优化“正例集合在候选集中的概率质量”。
- 增加 `CardinalityHead`（数量预测头）并支持 `--use-cardinality-head` 联合训练，保存到模型权重中。
- `Retriever` 增加 `use_khat` 推理开关（默认关闭），可在 learned scorer 可用时按预测 `k_hat` 截断结果。
- `scripts/eval_router.py` 增加 `--bootstrap`，输出 Recall@k / Top1 的置信区间，减少小幅波动误判。
- 新增 `scripts/data_collector.py` 与 `scripts/generate_training_samples.py`，打通交互日志到训练样本生成的最小闭环。

### 关键设计取舍
1. **默认行为不变**：新能力全部通过参数开关激活，旧命令可直接复用。
2. **最小侵入**：不改动现有 Router/policy 主流程，仅在训练目标与评测统计处加能力。
3. **可回滚**：listwise、ranked negatives、cardinality 均可单独关闭，便于逐步回归定位。

### 下一步建议
- 在安装 `torch` 的环境下对 pairwise/listwise 与 random/ranked 做系统对照，配合 bootstrap 报告判断显著性。
- 收集真实“补充追问”日志，批量生成 multi-positive 样本，逐步替代只含单正例的训练集。
- 在开启 `use_khat` 前，先以验证集统计 `k_hat` 的 MAE 分布，避免截断过度影响召回。

## 2026-02-17（ablation matrix 一键实验脚本）

### 本次变更摘要
- 新增 `scripts/run_ablation_matrix.py`，支持一条命令串行跑完 6 组配置 × 3 个 seed。
- 每个 `config + seed` 自动执行：训练（若需要）-> 评测（`eval_router.py` + bootstrap CI）-> 落盘日志与指标。
- 结果目录统一写入 `runs/<timestamp>/`，并生成 `summary.csv` 汇总关键指标。

### 如何运行
- 全量运行（默认 bootstrap=500，默认 seeds=11,29,47）：
  - `python scripts/run_ablation_matrix.py`
- 指定部分配置 / seeds（便于烟测）：
  - `python scripts/run_ablation_matrix.py --configs baseline --seeds 11 --bootstrap 20`

### 产物结构
- 总目录：`runs/<timestamp>/`
- 每组子目录：`runs/<timestamp>/<config_name>/seed_<seed>/`
  - `train.log.txt`：训练输出（baseline 组为说明文本）
  - `eval.log.txt`：评测输出原文
  - `eval.metrics.json`：解析后的关键指标
  - `weight_path.txt`：本次权重文件路径记录
  - `run.meta.json`：运行参数与文件索引
- 汇总表：`runs/<timestamp>/summary.csv`
  - 字段：`config_name, seed, Recall@5_mean, Recall@5_CI_low, Recall@5_CI_high, Top1_mean, MRR_mean`

### 配置说明
- 6 组配置分别为：
  - `baseline`
  - `pairwise_random`
  - `pairwise_ranked`
  - `listwise_random`
  - `listwise_ranked`
  - `listwise_ranked_cardinality`
- 其中 `listwise_ranked_cardinality` 仅在训练中开启 `use-cardinality-head`，评测不启用 `use_khat` 推理截断。

## 2026-02-18（run-dir/cache/config 统一规范）

### 本次变更摘要
- `scripts/eval_router.py` 新增 `--run-dir` 与 `--config`：
  - 传 `--run-dir` 时，写出 `eval.log.txt`、`eval.metrics.json`、`config.snapshot.json`
  - 默认配置可从 `configs/default.yaml` 读取，CLI 参数可覆盖
- `scripts/train_reranker.py` 新增 `--run-dir` 与 `--config`：
  - 传 `--run-dir` 时，写出 `train.log.txt`、`train.metrics.json`、`config.snapshot.json`
  - 训练权重优先写到 `run-dir/tiny_reranker.pt`，并兼容复制到 `--save-path`
- `scripts/run_ablation_matrix.py` 改为配置驱动：
  - 支持 `--config`、`--encoder-backend`，并透传到训练+评测
  - 每个 seed 结果写在 `runs/<timestamp>/<config>/seed_<seed>/`
  - 训练权重双落盘：`runs/.../tiny_reranker.pt` + `data/ModelWeights/<config>__seed_<seed>.pt`

### 缓存规范（核心）
- cache 文件改为签名隔离命名（dataset/backend/encoder/strategy/k/version）
- memory cache 与 eval cache 都写入签名头（第一行 `_meta.cache_signature`）
- 读取时签名不匹配会自动重建，避免跨配置误复用导致“能跑但不可信”

### 唯一推荐入口（批量实验）
- `python scripts/run_ablation_matrix.py`
- 如需强制后端：
  - `python scripts/run_ablation_matrix.py --encoder-backend hf`
  - `python scripts/run_ablation_matrix.py --encoder-backend simple`

### 兼容原则
- 未传 `--run-dir` 时，保持旧行为（可继续使用旧路径）
- 未传 `--config` 时，默认读取 `configs/default.yaml`

## 2026-02-19（run registry + cache 指纹 + 轻量模型管理）
### 本次变更摘要
- 新增全局实验索引：`runs/registry.csv`。
- `scripts/eval_router.py`、`scripts/train_reranker.py`、`scripts/run_ablation_matrix.py` 在写入 run-dir 后会幂等 upsert 到 registry（按 `run_dir` 去重更新）。
- 新增 `scripts/list_runs.py`：按 dataset/backend/loss_type 等筛选最近 runs。
- 新增 `scripts/select_best_run.py`：按指标（默认 `Recall@5`）选最优 run，并落到 `runs/best/<dataset>/<encoder_backend>/`。
- 新增 `scripts/clean_cache.py`：按 dataset/backend/签名片段选择性清理 `data/VectorCache`。
- cache 签名增强：在 dataset/backend/encoder/strategy/k 基础上加入数据指纹（memory/eval 文件 sha1 short），并提升为 `cache_v=v2`，避免“文件内容变了但路径没变”的错复用。
- 训练权重写入策略调整：
  - 传 `--run-dir` 时默认只写 `run-dir/tiny_reranker.pt`。
  - 不再默认覆盖 `data/ModelWeights/tiny_reranker.pt`。
  - 仅在显式传 `--save-path` 或 `--export-weights-to` 时额外导出。
- README 追加了推荐入口：`list_runs` / `select_best_run` / `clean_cache` / best 权重复现 eval 命令。

### 本次新增/修改文件
- 新增：`scripts/list_runs.py`
- 新增：`scripts/select_best_run.py`
- 新增：`scripts/clean_cache.py`
- 增强：`scripts/runtime_utils.py`（registry upsert、git commit、data fingerprint）
- 修改：`scripts/eval_router.py`
- 修改：`scripts/train_reranker.py`
- 修改：`scripts/run_ablation_matrix.py`
- 追加：`README.md`
- 分析：`analysis/system_readiness_2026-02-18.md`

### 烟测记录（本地终端）
- 语法检查：

## 2026-02-20（数据管道纲领对齐：用户语料优先）

### 本次对齐摘要
- 明确了数据处理总纲（先对齐原则，再做实现细节）：
  1) 优先提取用户发言；
  2) 先规则清洗明显复制粘贴大段内容（代码块仅为例，不限于代码）；
  3) 必须保持会话内顺序；
  4) 先按对话窗口分段，再用句向量相似度做话题断裂补充；
  5) 同一事件归为 cluster，cluster 内互为候选正例；
  6) 先规则法打底，再决定是否引入 LLM 摘要。

### 结构化建议（已写入 README）
- 中间产物建议固定为：
  - `data/Processed/user_turns_raw.jsonl`
  - `data/Processed/user_turns_dedup.jsonl`
- 最小字段：
  - `session_id/turn_id/role/text/timestamp`
  - `user_message_clean/topic_break_flag/cluster_id/candidate_pool_ids`

### 关键实现约束
- 顺序不可破坏：任何清洗/聚类都不能改写原 turn 时序。
- 话题断裂先用启发式阈值（建议起点：相邻 user turn 相似度 < 0.35 且新关键词明显），后续再调参。
- 训练向约束：任一 cluster 内样本均可单独作为 query；后续必须加入 query 多表达增强，避免单一表述过拟合。

## 2026-02-21（新增聊天记忆处理器并列包）

### 本次变更摘要
- 新增并列包：`src/chat_memory_processor/`
  - `extractor.py`：从 DeepSeek 风格导出中提取用户发言
  - `cleaning.py`：规则清洗与大段复制粘贴检测
  - `segmentation.py`：相邻 turn 相似度 + 新词比例做话题断裂候选
  - `pipeline.py`：保序、聚集、双文件输出
  - `models.py`：用户 turn 数据结构
- 新增脚本：`scripts/chat_memory/build_user_turns.py`
  - 默认读取 `data/RawDeepseekChats/conversations_sample.json`
  - 默认输出
    - `data/Processed/user_turns_raw.jsonl`
    - `data/Processed/user_turns_dedup.jsonl`

### 为什么并列而不是塞进 memory_indexer
- `memory_indexer` 关注检索/路由/打分；
- `chat_memory_processor` 关注原始聊天语料处理；
- 两者并列后，边界清晰、依赖方向单一，后续替换任一模块都更稳。

## 2026-02-21（切回 conversations.json 主路径）

### 本次调整
- 明确将 `data/RawDeepseekChats/conversations.json` 作为正式输入源（标准 JSON）。
- 删除/回退仅针对 sample 截段格式的非常规解析分支，避免长期维护无效兼容逻辑。
- `build_user_turns.py` 默认输入改为 `conversations.json`。
- 抽取逻辑改为“按 conversation 处理 + 会话内保序 + 会话内聚集”，不跨会话混切。

## 2026-02-21（讨论留痕：自动切分 + 聚类 + 低成本 query 反推）

### 本次共识
- 继续坚持：同一事件 cluster 内互为候选正例，服务 listwise 排序。
- 话题切分不建议长期依赖固定阈值；应优先“会话内自适应阈值/变化点检测”。
- query 扩展不强依赖 LLM：先做模板+词法+同义替换的低成本反推方案。
- 跨会话聚合不禁用，但建议后置为二阶段增强能力。

### 文档产物
- 新增：`README/话题切分与聚类方案.md`
  - 给出默认执行路径、接口预留、与 listwise 的衔接方式。
  - `python -m py_compile scripts/runtime_utils.py scripts/train_reranker.py scripts/eval_router.py scripts/run_ablation_matrix.py scripts/list_runs.py scripts/select_best_run.py scripts/clean_cache.py`
- 最小训练（simple/followup）：成功写入 `runs/tmp_registry_train/tiny_reranker.pt`。
- 最小评测（simple/followup）：成功产出 `runs/tmp_registry_eval/eval.metrics.json`。
- ablation 最小对照（pairwise_ranked, seed=11）：成功生成 `runs/<timestamp>/summary.csv`。
- registry 幂等验证：同一 `run_dir` 重跑不会重复新增行。
- best 选择验证：`select_best_run` 可生成 `runs/best/followup/simple/`。

### 备注
- 目前项目里仍有历史 `v1` cache 文件；建议先用 `clean_cache --dry-run` 查看再定向清理。
- 后续新增/修改脚本时，开发日志会同步追加更新。

## 2026-02-19（编码器前缀修复 + runtime 固化入口）
### 本次变更摘要
- 修复 `HFSentenceEncoder` 的 E5 前缀开关：
  - `src/memory_indexer/encoder/hf_sentence.py` 的 `_maybe_prefix()` 现在尊重 `self.use_e5_prefix`，并仅在 `model_name` 包含 `e5` 时添加 `query:/passage:` 前缀。
  - 保持 `_resolve_local_files_only()` 逻辑不变（默认离线策略未改）。
- 为 `E5TokenEncoder` 增加软护栏，避免双前缀：
  - `src/memory_indexer/encoder/e5_token.py` 的 `encode_tokens()` 现在会先检查输入是否已以 `query:` 或 `passage:` 开头；若已带角色前缀则不再二次 `_maybe_prefix()`。
  - 避免未来 `use_e5_prefix=True` 场景出现 `query: passage: ...` 的角色错乱。
- 占位 token vec 风险处理：
  - 已排查主链路（`eval_router/train_reranker/generate_training_samples` 的 hf 路径）均显式传 `token_encoder=E5TokenEncoder`。
  - 在 `HFSentenceEncoder.encode_tokens()` 中加入 trace 模式一次性 warn（`MEMORY_TRACE=1` 时），提醒该方法返回的是占位 token vec，正式实验应使用 E5TokenEncoder。
- 新增 runtime 可执行入口（替代手动 IDE 运行配置）：
  - `scripts/runtime/make_followup_eval.py`
  - `scripts/runtime/eval_hf_normal.py`
  - `scripts/runtime/ablate_followup_listwise_vs_pairwise.py`
  - `scripts/runtime/select_best_followup_hf.py`
  - 每个脚本启动时都会打印提醒：可用 `list_runs/select_best_run` 查看与挑选最佳实验。
  - runtime 脚本会优先使用项目 `.venv/Scripts/python.exe`（若存在），否则回退 `sys.executable`。
- `run_ablation_matrix` 增强：
  - 新增 `--run-dir`，允许固定输出到精确目录（不再强制追加 timestamp）。
- 修复一个运行期兼容问题：
  - `scripts/runtime_utils.py` 的 `Tee` 新增 `isatty()`，避免 `transformers` 在日志输出阶段触发 AttributeError。

### 最小验证（B：eval_hf_normal）
- 实际执行：`python scripts/runtime/eval_hf_normal.py`
- 结果：已成功启动 HF/E5 链路并进入评测，日志中出现关键指标：
  - `Recall@k=0.780`
  - `coarse_lexical_jaccard=0.146`
- 结论：无“世界观突变”迹象（指标量级与既有预期一致）。
- 备注：完整全量评测在当前终端会超过单次命令超时窗口，但启动与核心指标已确认正常。

## 2026-02-19（快速止血：runtime 参数固化 + 训练默认修正 + 防覆盖备份）
### 本次变更摘要
- `scripts/runtime/eval_hf_normal.py` 已按常用命令固定关键参数，不再依赖 default.yaml 的隐式默认：
  - `--hf-local-only`
  - `--top-n 30 --top-k 5`
  - `--policies half_hard,soft`
  - `--candidate-mode union`
  - `--ablation-groups baseline(auto),S-only,L-only`
  - `--use-learned-scorer --reranker-path data/ModelWeights/tiny_reranker.pt`
  - `--no-consistency-pass`
- `configs/default.yaml` 训练默认值调整：`train_reranker.epochs` 从 3 改为 10。
- `scripts/train_reranker.py` 新增轻量防覆盖保险丝：
  - 当目标写入路径为 `data/ModelWeights/tiny_reranker.pt` 且该文件已存在时，先自动备份为
    `data/ModelWeights/tiny_reranker.bak_YYYYmmdd_HHMMSS.pt`，再覆盖写入。

### 影响
- 运行 `python scripts/runtime/eval_hf_normal.py` 会直接产出你熟悉的评测视图（组别/策略/检索配置固定）。
- 误训练覆盖风险降低：即便直接写默认权重，也会先留一份时间戳备份。

## 2026-02-19（A-E 一次性改造：listwise/pairwise 正式对照主线）
### A) 生成规则去 user_feedback 硬依赖
- `scripts/generate_training_samples.py` 已移除 followup 对 user_feedback 关键词的硬门槛。
- 新默认：`turn_index > 0` 且 `curr_turn.cited_mem_ids` 非空就产样本。
- `positives`：优先 `prev_cites ∩ curr_cites`，若空则用 `curr_cites`。
- `hard_negatives`：`prev_cites - positives`（可空）。

### B) 增加“从 eval 合成 followup”通路
- `scripts/generate_training_samples.py` 新增 `--from-eval-dataset <name>`。
- 可直接从 `eval_normal.jsonl` 合成 followup 样本，再补全 candidates/hard_negatives，输出到 `data/Processed/eval_followup.jsonl`。
- `scripts/runtime/make_followup_eval.py` 同步支持：
  - `--from-eval-dataset normal`（不依赖真实日志）
  - 或 `--log-path ...`（真实日志）

### C) 训练入口拆分 + 权重正规化
- 新增 `scripts/train_pairwise_reranker.py`：固定 `loss_type=pairwise`、`neg_strategy=ranked`，默认写 `data/ModelWeights/pairwise_reranker.pt`。
- 新增 `scripts/train_listwise_reranker.py`：固定 `loss_type=listwise`、`neg_strategy=ranked`，默认写 `data/ModelWeights/listwise_reranker.pt`。
- 两个 wrapper 在覆盖前都会自动备份 `.bak_时间戳`。

### D) 显式对比 runtime 脚本
- 新增 `scripts/runtime/compare_followup_pairwise_vs_listwise.py`：
  1) 调 `train_pairwise_reranker`
  2) 调 `train_listwise_reranker`
  3) 分别调用 `eval_router --use-learned-scorer --reranker-path ...`
  4) 终端打印 Recall@5/Top1/MRR 对比表

### E) eval_router 增加权重与 meta 可见性
- `scripts/eval_router.py` 在 `--use-learned-scorer` 时会打印：
  - 当前加载的权重路径
  - 权重内 `meta`（loss_type/neg_strategy/epochs/seed/dataset，若存在）

### 实测
- 已用 `simple` 路径执行完整对照：
  - `python scripts/runtime/compare_followup_pairwise_vs_listwise.py --encoder-backend simple --epochs 1 --top-n 20 --top-k 5 --bootstrap 5 --seed 11`
- 成功输出对比表与产物目录：
  - `runs/rt/compare_followup_pairwise_vs_listwise/<timestamp>/...`

### 正式执行（HF/E5）推荐顺序
1) 先合成/重建 followup 数据（不依赖日志）：
   - `python scripts/runtime/make_followup_eval.py --from-eval-dataset normal --encoder-backend hf --top-n 30 --candidate-mode union`
2) 再跑显式对照：
   - `python scripts/runtime/compare_followup_pairwise_vs_listwise.py --encoder-backend hf --epochs 10 --top-n 30 --top-k 5 --bootstrap 200 --seed 11`

## 2026-02-19（compare 默认减负 + 训练设备默认 GPU）
### 本次变更摘要
- `scripts/runtime/compare_followup_pairwise_vs_listwise.py`：
  - 默认 `--bootstrap=0`（不做 bootstrap 重采样）。
  - 默认不训练；新增 `--train` 才会触发 pairwise/listwise 训练。
  - 默认不训练时会检查权重文件是否存在，不存在直接报清晰错误。
  - 新增 `--device`（仅 `--train` 时生效），用于显式传训练设备。
  - 代码内加入注释，说明为何默认关闭 bootstrap、为何默认跳过训练（这是踩坑位）。
- `scripts/train_reranker.py`：
  - 训练默认设备从配置读取时默认值改为 `cuda`。
  - 增加设备自动降级逻辑：
    - `device=auto/gpu/cuda` 时：有 GPU 用 CUDA，无 GPU 自动回退 CPU。
    - 显式请求 `cuda*` 但机器无 GPU 时会打印 warn 并回退 CPU。
  - 代码内加入注释，说明这个策略是为避免“默认 CPU 巨慢”坑。
- `configs/default.yaml`：
  - `train_reranker.device` 从 `cpu` 改为 `cuda`。

### 影响
- 对比脚本现在默认更接近“只评估，不重复训练”的使用习惯。
- 训练默认倾向 GPU，降低误用 CPU 导致的超慢问题。

## 2026-02-20（新增一键训练：pairwise + listwise）
### 新增脚本
- `scripts/runtime/train_pairwise_and_listwise_reranker.py`

### 设计目标
- 避免每次手敲一长串参数。
- 一次命令顺序训练：
  1) `scripts.train_pairwise_reranker`
  2) `scripts.train_listwise_reranker`
- 统一产物目录：`runs/rt/train_pairwise_and_listwise/<timestamp>/...`
- 固定权重路径（便于后续 eval/compare）：
  - `data/ModelWeights/pairwise_reranker.pt`
  - `data/ModelWeights/listwise_reranker.pt`

### 关键参数与坑位处理
- `--profile formal|quick`：
  - `formal` 默认 `epochs=20, top_n=30`
  - `quick` 默认 `epochs=3, top_n=20`
- `--device` 默认 `cuda`（底层 train_reranker 已支持无 GPU 自动降级到 CPU）
- `--reset-weights`：训练前删除这两个正式权重（不删 `.bak`）
- 默认 `hf-local-only`（除非显式 `--hf-online`）

### 推荐使用
- 正式训练（HF/E5）：
  - `python scripts/runtime/train_pairwise_and_listwise_reranker.py --encoder-backend hf --profile formal --seed 11 --reset-weights`
- 快速冒烟：
  - `python scripts/runtime/train_pairwise_and_listwise_reranker.py --encoder-backend simple --profile quick --epochs 1 --top-n 20 --seed 11 --reset-weights`

## 2026-02-20（compare 报错修复 + 日志降噪）
### 修复
- `scripts/runtime/compare_followup_pairwise_vs_listwise.py`：
  - run-dir 全部改为绝对路径（基于 `REPO_ROOT`），避免在不同工作目录下解析失败导致
    `FileNotFoundError: .../eval.metrics.json`。
  - 在读取评测结果前新增存在性检查，缺失时给出明确提示并指向对应 `eval.log.txt`。

### 日志降噪
- compare 子进程默认注入环境变量：
  - `TQDM_DISABLE=1`
  - `HF_HUB_DISABLE_PROGRESS_BARS=1`
- 目的：减少 `Loading weights ...` 这类第三方进度条噪音。

### 结论核验
- pairwise/listwise 权重文件不是同一个：
  - `pairwise_reranker.pt` 与 `listwise_reranker.pt` 的 SHA1 前缀不同。
- 但当前一次实测指标完全一致，属于“不同模型在当前数据/候选分布上学到近似排序边界”的结果，并非路径误用。

## 2026-02-22（query 扩充方案共识留痕：processor 侧）
### 本次共识（先定方向，后实现）
- 生成策略从“纯规则优先”调整为“LLM teacher 主导 + 规则兜底”。
- API 侧采用 OpenAI 兼容格式；默认优先低成本 provider（DeepSeek），保留 ChatGPT 作为可切换 teacher 基线。
- 现有 `data/Processed/eval_chat_followup.jsonl` 中的“笨规则 query 生成”视为过渡产物，后续可整体覆盖。
- query 扩充的唯一事实源优先使用 `data/Processed/memory_chat.jsonl`（避免 memory/eval 双表并行漂移）。
- 原始 memory 文本“一字不改”可直接作为 query 候选（这是基础通道，不应丢弃）。
- 扩充 query 的价值定位：覆盖用户“重问同问题”之外的表达变体（如“你还记得那次…/突然很难受”等）。

### 三层 query 扩充（确认继续）
- 事件层（cluster outline）：围绕簇级事件主线生成概括型 query。
- 细节层（cluster details）：围绕实体/动作/时间/对象生成具象 query。
- 状态层（emotion/value）：围绕情绪、价值判断、偏好变化生成状态型 query。
- 上述三层均可由 teacher 一次生成后再做本地过滤与去重。

### 桥接（bridge）阶段判断
- 跨簇桥接暂不并入当前迭代，放到 query 扩充稳定后再做。
- 先把“簇内可召回 + 三层覆盖率 + 噪声控制”做稳，再考虑跨簇联想链路。

### 聚类与 indexer 的边界
- processor 负责产出聚类与 query 扩充结果。
- indexer 不应把聚类当硬约束；可作为弱特征或先验信号使用。

### 负例策略（当前保守版本）
- 先接受“远簇随机负例”作为默认负例来源。
- “临近不同簇负例”与“语义近但标签冲突负例”暂不强推，避免过度信任分段/聚类质量。
- 现有 `hard_negatives` 字段保留，但后续规则将以新方案重算。

### 术语澄清（语义近但标签冲突）
- 这里的“标签”指任务语义标签而非模型类别标签，例如：事件类型、意图层级（事件/细节/状态）、情绪极性、是否同一事件链。
- 语义字面接近但标签不一致的样本，可作为高难负例；是否启用延后到下一轮实验决策。

## 2026-02-22（query 直通基线落地 + 生成步骤拆分）
### 本次变更摘要
- 取消 processor 内置的模板化“笨规则 query 生成”主路径，改为**identity 通道**：
  - `query_text = memory.text`
  - `positives = [mem_id]`
- 新增独立脚本：`scripts/memory_processer/build_chat_queries.py`
  - 输入：`data/Processed/memory_chat.jsonl`
  - 输出：`data/Processed/eval_chat_followup.jsonl`、`data/Processed/eval_chat.jsonl`
  - 可选：`--supplemental-queries` 从外部 JSONL 追加补充 query（不覆盖 identity 基础通道）。
- `scripts/memory_processer/build_chat_memory.py` 职责收敛为仅生成：
  - `user_turns_raw.jsonl`
  - `user_turns_dedup.jsonl`
  - `memory_chat.jsonl`
  - 并提示 query/eval 需单独运行 `build_chat_queries.py`。

### 结果核验
- 已按新流程重生：
  - `data/Processed/eval_chat_followup.jsonl`
  - `data/Processed/eval_chat.jsonl`
- 一致性检查：
  - `followup_ok=3428 followup_bad=0`
  - `eval_ok=3428 eval_bad=0`
  - 即每条 query 都与其唯一正例 memory 文本一一对应。

## 2026-02-22（讨论留痕：LLM 补充 query 设计，Object/Stance/Intent）
### 结论（本轮讨论）
- 方案总体合理，可作为下一轮实现基线：
  - 保持 identity query 为主通道（已落地）。
  - LLM 生成 query 仅为补充通道，独立文件追加，不覆盖 identity。
- 三层从“事实/细节/情感”调整为“对象/立场/意图”是正确收敛：
  - 对象层：语义对象及模糊追问（“我们之前说过那个……叫什么”）。
  - 立场层：态度保持/反转/摇摆（“我好像改主意了”）。
  - 意图层：对 AI 的行为目标（求解释、求方案、求辩论、求安慰等）。
- 细节层可暂时移除（词法与关键词召回已覆盖一部分细节表达）。

### 数量策略（采纳）
- 若 cluster 含 `n` 条 memory：
  - 每层目标条数 `k = ceil(n / 3)`；
  - 三层总量约 `3k`，与 `n` 同量级；
  - identity 仍保留 1:1，不参与此配额。

### 生成与容错（采纳）
- 单层批量生成（每次请求只生成同一层 `k` 条），提高风格一致性与可控性。
- 严格 JSON schema 校验：
  - 字段/数量/去重/长度不合格则重试；
  - 最多重试 3 次；
  - 仍失败则回退到同口径模板填充（模板与提示词语义一致）。

### 工程边界（采纳）
- 补充 query 从 `memory_chat.jsonl` 单源生成，避免与 eval 双源漂移。
- bridge（跨簇桥接）仍后置，不进入当前迭代。
- 负例策略当前继续保守：默认远簇随机，其他 hard negative 等后续评估再开。

### API 与密钥管理（安全约束）
- 用户在对话中给出的明文 key 视为已暴露，建议立即轮换（revoke + regenerate）。
- 仓库内推荐做法（本地文件 + git ignore）：
  - 例如 `data/_secrets/deepseek.env`（仅本地）；
  - `.gitignore` 增加：`data/_secrets/`、`*.env.local`。
- 运行时读取优先级建议：
  1) 环境变量 `DEEPSEEK_API_KEY`
  2) 本地 secrets 文件
  3) 缺失时报错退出

### 资料核对（官方文档）
- DeepSeek 支持 OpenAI 兼容 API，推荐用 `https://api.deepseek.com` 作为 `base_url`，鉴权用 `Authorization: Bearer <key>`。
- 可用聊天模型包括 `deepseek-chat`、`deepseek-reasoner`（按任务选择）。

## 2026-02-22（实现落地：并发 LLM 补充 query 生成器）
### 实现范围
- 新增模块：`src/chat_memory_processor/llm_querygen.py`
  - 输入：`memory_chat.jsonl` rows（按 `cluster_id` 聚合）
  - 输出：补充 query rows（JSONL 结构兼容 `build_chat_queries.py --supplemental-queries`）
  - 三层：`object` / `stance` / `intent`
  - 数量：每层 `k = ceil(n/3)`（`n` 为 cluster memory 数）
  - 并发：异步批量请求（`aiohttp + asyncio.as_completed + semaphore`）
- 新增脚本：`scripts/memory_processer/build_chat_supplemental_queries.py`
  - 默认输出：`data/Processed/eval_chat_supplemental.jsonl`
  - 默认读取：
    - `output.memory_out` 作为输入 memory
    - `query.llm.*` 作为 LLM 配置
  - 支持 `--fallback-only`（不请求 API，仅模板兜底，便于离线验证）。

### 提示词工程（按讨论落实）
- 层级定义明确区分：对象/立场/意图。
- 模板只作参考，提示中显式要求“高自主度（>=60% 非模板复用表达）”。
- 模板顺序与“选择指导卡片”顺序在每次请求中重排，降低固定首模板吸附。
- 禁止元话术：训练数据/样本/cluster/召回/模型/向量等。
- 输出严格 JSON 对象：`{"queries":[{"query_text":"..."}]}`。

### 稳定性与兜底
- 校验失败（格式/条数/禁词/重复）会重试，最多 `max_retries=3`。
- 三次失败后按同层模板自动填充，确保每层 `k` 条可交付。
- 当前补充 query 默认 `positives` 为该 cluster 全部 `mem_id`，`hard_negatives` 先留空。

### 文件与配置
- `configs/chat_memory.yaml` 增加：
  - `query.llm_supplemental_out`
  - `query.llm.{base_url,model,api_key_file,concurrency,timeout_seconds,max_retries,temperature,top_p,seed,max_clusters,min_cluster_size}`
- `.gitignore` 增加：`/data/_secrets`
- 新增示例：`configs/deepseek.env.example`

### 执行链路（当前推荐）
1. `build_chat_memory.py` 生成 memory 基础产物。
2. `build_chat_supplemental_queries.py` 生成补充 query（独立文件）。
3. `build_chat_queries.py --supplemental-queries ...` 合并补充与 identity（identity 不覆盖）。

### 最小在线验证（DeepSeek）
- 运行（限 1 个 cluster）：
  - `python scripts/memory_processer/build_chat_supplemental_queries.py --config configs/chat_memory.yaml --max-clusters 1 --concurrency 16 --out data/Processed/eval_chat_supplemental.api_preview.jsonl`
- 结果：
  - `supplemental_rows=42`
  - 覆盖三层（object/stance/intent）并满足 JSON 输出约束。

## 2026-02-22（提示词工程优化：降模板感 + 强锚点约束）
### 背景问题
- 首版小样出现明显模板痕迹（固定句式重复）。
- 存在“空槽位”风险（如“把什么梳理”“反方辩什么”）。
- 部分 query 句首口癖过强（`哈哈/不不不/等等`）。

### 本次优化
- `src/chat_memory_processor/llm_querygen.py`：
  - 强化 prompt 约束：
    - 模板仅作参考，禁止直接套用原句。
    - 每条 query 必须命中对象锚点，不允许空泛代词单独承载对象。
    - 明确禁止元话术和口癖句首。
  - 参考模板改为“骨架描述”而非完整可照抄句。
  - 新增后处理硬校验：
    - 句首口癖过滤、模板化前缀过滤、锚点命中检查。
  - 生成策略改为“LLM 优先 + 部分兜底补齐”：
    - 不再重试失败后整层直接 fallback；
    - 改为跨重试累计 LLM 有效结果，不足部分才补 fallback；
    - `meta.generation_mode` 标记 `llm/mixed/fallback`。

### 当前观察
- 同样 `max-clusters=1` 验证下，产物以 `generation_mode=llm` 为主，模板复读显著下降。
- 仍有少量锚点噪声受原始记忆内容影响（属于下一步锚点清洗可继续优化项）。

## 2026-02-22（排查结论：42 条来源与规则软化）
### 用户反馈定位
- `eval_chat_supplemental.api_preview.jsonl` 中出现“开头重复高”“模板感重”。
- 质疑 1-cluster 结果为何是 42 条而非预期 18 条。

### 排查结论
- 非多次运行叠加，不是重复写入问题。
- 当时逻辑为：每层 `k = ceil(n/3)` 且无上限；该 cluster 的 `n=40`，所以 `k=14`，三层共 `42` 条。

### 本次修正
- 新增每层上限参数：`max_per_layer`（默认 6）。
  - 结果：`n=40` 时，`k=min(ceil(40/3), 6)=6`，三层共 18 条。
- 按用户意见软化过度死板约束：
  - 去掉“句首禁止口语词”的硬限制；
  - 将“不得只用代词”改为建议性约束；
  - 保留“禁止直接套用模板原句”与“必须出现对象锚点”。
- 增加锚点过滤，减少“我就是说/还是那句话”这类话语标记进入对象锚点池。

## 2026-02-22（可上传版本总结：LLM 补充 query 收敛）
### 本次定版点
- `chat_memory_processor` 形成“两通道”：
  - identity 通道：`memory.text -> query_text`（1:1 基础通道）
  - supplemental 通道：LLM 并发生成 `object/stance/intent` 三层补充 query（独立文件追加）
- 生成流程拆分完成：
  1) `build_chat_memory.py` 仅产出 raw/dedup/memory
  2) `build_chat_supplemental_queries.py` 产出补充 query
  3) `build_chat_queries.py --supplemental-queries ...` 合并输出 eval/followup

### 关键修正
- 并发补充 query 支持重试与兜底，且以 LLM 结果优先（不足才补 fallback）。
- 明确不是多次运行叠加：此前 42 条来自 `k=ceil(n/3)`；现已加入 `max_per_layer`（默认 6）。
- 提示词工程从“硬模板”收敛到“模板参考 + 禁止原句照抄 + 对象锚点约束”。

### 当前验证结果（api preview）
- 单簇验证产物：`data/Processed/eval_chat_supplemental.api_preview.jsonl`
- 条数：18（`object=6` / `stance=6` / `intent=6`）
- 生成模式：`generation_mode=llm` 全量命中。
