一套可扩展的接口与数据格式设计（MVP 可跑 → 逐步增强）
0. 总体原则（你只需要记住这三条）

原文永远保存：向量会过时，原文不会。

向量表示可版本化：同一条记忆可以有多个 embedding 版本/策略。

检索分两层：粗召回（快）+ 精排（准）。MVP 先做最简单的粗召回也行。

1) 核心对象与字段（统一数据模型）
1.1 MemoryItem（记忆条目）

这是库里的“条”。不管你后面怎么扩展，它都不变。

建议存成 JSONL（每行一个 JSON），或者 SQLite 表。

必备字段：

mem_id：字符串ID（uuid 或递增）

text：原文（必须有）

created_at：时间戳（可选）

source：来源（chat/log/web/manual）

thread_id：对话线程/主题ID（可选）

turn_range：来自对话的哪几轮（可选）

tags：你手工/自动标签（可空）

meta：自由字典（比如说话人、地点、情绪等）

可选但强烈建议：

summary：一句话摘要（MVP 可先空，后面补）

keywords：关键词列表（MVP 可用简单 TF-IDF 产）

重点：MemoryItem 是“内容层”，和任何向量算法解耦。

1.2 EmbeddingRecord（向量记录）

同一条记忆可以有多个“向量版本”：比如不同模型、不同切词策略、不同“向量组数量”等。

字段：

emb_id：向量记录ID

mem_id：对应的记忆条目

encoder_id：编码器版本标识（比如 "bge-m3@v1"、"bert-base-chinese@v2"）

strategy：策略名（比如 "single_vec"、"token_pool_top32"、"cluster_centers_6"）

dims：维度（例如 768/1024）

n_vecs：向量条数（单向量=1；向量组可能=32/64/6等）

vecs：向量内容（不建议直接塞 JSON；建议二进制存 FAISS / npy / mmap）

aux：辅助信息（比如保存了哪些 token 的位置、对应词字符串等，做可解释日志）

重点：EmbeddingRecord 是“表示层”，允许你未来换模型、换策略、加训练，都不破坏内容层。

1.3 Query（查询对象）

一次检索请求（输入 x）也要结构化，否则后面训练、评估全乱。

字段：

query_id

text

context：可选，上下文窗口

encoder_id：用哪个编码器

strategy：用哪个 query 生成策略（要和记忆策略兼容）

q_vecs：生成的 query 向量组（同样建议不直接塞 JSON）

created_at

2) 组件接口（模块边界清晰，MVP 也能跑）

你按下面的“接口”写代码，先实现最小版本，后面只加实现类，不改调用方。

2.1 Encoder（编码器）

职责：text -> token_vecs / sentence_vec

接口建议：

encode_sentence(text) -> vec(d)

encode_tokens(text) -> (token_vecs[n,d], tokens[n])

MVP 实现：

用现成中文句向量模型做 encode_sentence

用 BERT 做 encode_tokens

扩展实现：

换更强模型（bge/e5/gte）

加 Adapter/LoRA 微调后仍然复用接口

2.2 Vectorizer（向量组构造器）

职责：把 encoder 的输出变成“单向量/向量组”。

接口建议：

make_memory_vectors(text, encoder) -> EmbeddingRecord

make_query_vectors(text, encoder) -> QueryVecs

策略例子（全是可插拔的）：

single_vec: 句向量或 token 平均（粗但快）

token_pool_topK: 挑 topK token（用 TF-IDF/停用词/词性）

cluster_centers_r: token 向量聚类取 r 个中心（你想要的 3~8 个“场方向”）

hybrid: “1 个粗句向量 + 32 个 token 向量组”（粗召回+精排两用）

MVP 推荐你直接用：

记忆：token_pool_top32（32个向量就很够用）

查询：token_pool_top32 或 cluster_centers_6（更像场）

2.3 Index（索引）

职责：存向量、做粗召回（ANN）或倒排检索（BM25）。

接口建议：

add(emb_record)

search(query_vec, top_n) -> candidate_ids + coarse_scores

save()/load()

MVP 现实做法（适合你 2060 + 十几小时预算）：

粗召回用 单向量 ANN：FAISS（CPU/GPU都行）

每条记忆存一个 coarse_vec（句向量或向量组的均值）

精排时再用向量组匹配（不进 FAISS）

扩展做法：

加 BM25 倒排做混合召回（召回率更稳）

加多索引：不同 encoder_id/strategy 各有一个索引

2.4 Scorer（精排打分器）

职责：对候选集合做“向量组相似度”精排。

接口建议：

score(query_vecs, memory_vecs) -> float

rank(query_vecs, candidate_emb_records) -> sorted list

MVP 推荐的打分（简单、好用、符合你“某方面对齐就算赢”）：

对每个 q：找 M 里最大余弦 maxsim(q, M)

得到一串分数后：取 top-3 平均作为总分（避免偶然撞词爆分）

扩展：

加权（按 token 重要性）

加交叉编码器 reranker（只对 top-200 跑）

2.5 Trainer（可选训练器，后面再上）

职责：用 (x,y) 自监督训练小东西。

接口建议：

build_pairs(dialogue) -> (x,y) 数据管线

train_projection_head(pairs) 训练小投影头（MLP）

train_reranker(pairs) 训练重排器（小模型）

你现在先把 Trainer 留空都行，因为 MVP 不靠它也能检索。

3) 文件/目录结构建议（利于扩展，不会变成屎山）
data/
  memory.jsonl              # MemoryItem 原始库
  pairs.jsonl               # (x,y) 训练对（可选）
  eval.jsonl                # 小评测集（你手工挑20~50条）

embeddings/
  bert_tokenpool32/
    meta.json               # encoder_id, strategy, dims, n_vecs...
    vectors.f16             # 二进制向量（或 npy/mmap）
    aux.jsonl               # token位置/关键词等可解释信息（可选）
  coarse_sentence/
    faiss.index             # 粗召回索引
    id_map.jsonl            # index->mem_id 映射

models/
  projection_head.pt        # 小投影头（可选）
  reranker.pt               # 小重排器（可选）

logs/
  retrieve_debug.jsonl      # 每次检索的命中解释日志

4) MVP 的具体实施步骤（按顺序干，别跳）
Step A：建库（内容层）

把你的对话/日志整理成 memory.jsonl

每条一个 mem_id，存 text 和基本 meta

Step B：做粗向量与粗索引（能跑起来的关键）

用句向量模型对每条记忆生成 coarse_vec

用 FAISS 建一个 ANN 索引（top-1000 召回）

到这一步，你已经能做到：输入一句话 → 找出语义相近的记忆（虽然不够“联想”，但能跑）。

Step C：做向量组（你真正想要的精排）

用 BERT token 向量 + token_pool_top32 给每条记忆生成 M_vecs

查询时生成 Q_vecs

对粗召回的 top-1000 逐条精排，输出 top-k

这就是“无训练 MVP”：用预训练模型 + 向量组匹配，已经比“单向量RAG”灵活很多。

Step D：可解释日志（强烈建议做）

每次检索输出：

top-k 记忆

分数

哪几个 query token 与记忆 token 对齐贡献最大（你会靠这个调策略）

5) 你担心的“要跑十几个小时”用在哪（合理分配）

建库向量化（一次性离线）：几小时到十几小时，完全合理

检索在线：粗召回毫秒级；精排 top-1000 * (32x32 余弦) 也能做到可接受（尤其你可以先 top-300 精排）

建议的性能旋钮：

coarse_topN：先 300，不够再 1000

n_vecs：先 16 或 32，别上来 64

相似度聚合：top-3 mean（很稳）

6) 后续扩展怎么无痛接入（这就是你要的“兼顾扩展”）

你以后要加的东西，都能塞进现有接口：

你要“索引场更像场”：把 Query 的 Vectorizer 从 token_pool 换成 cluster_centers_6

你要“更懂关系而非相似”：加一个 RerankerScorer，只对 top-200 跑

你要“自监督提升”：Trainer 训练一个小 projection_head，然后 Encoder 输出后过一层 head（接口不变）

你要“个性化”：把 projection_head 做成用户专属文件即可（每人几MB）