我的终极目标一直是：训练一个映射

- 文本 $x$ → 索引向量组/索引场 $Q(x)$

让它更像“该唤醒什么”的检索指令，而不是“这段话大概讲啥”。

我把“训练 $x → Q$”放到 MVP 之外的原因只有一个：先把检索闭环跑通，才能知道训练到底该优化什么。否则我会在没有仪表盘的情况下拧螺丝，拧到天荒地老也不知道方向对不对。

下面是一份可扩展到训练版的 Python 伪代码骨架：MVP 先用“预训练编码器 + 向量组构造”直接产出 $Q$；训练版再在 `QueryModel` 里加一个可训练的小头（projection head / adapter），让 $Q$ 变成“学出来的”。  
**新增路由思路**：先做“软路由”——所有候选仍可见，但用权重衰减贡献；后面再逐步走“半硬/硬路由”，让选错不可修复。

```python
# =========================
# Data structures
# =========================

from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
import numpy as np
import time

Vector = np.ndarray  # shape: (d,)
VectorGroup = np.ndarray  # shape: (n_vecs, d)

@dataclass
class MemoryItem:
    mem_id: str
    text: str
    created_at: float = field(default_factory=lambda: time.time())
    source: str = "unknown"
    meta: Dict = field(default_factory=dict)

@dataclass
class EmbeddingRecord:
    emb_id: str
    mem_id: str
    encoder_id: str
    strategy: str
    dims: int
    n_vecs: int
    vecs: VectorGroup                   # (n_vecs, d)
    coarse_vec: Optional[Vector] = None # (d,), for coarse recall
    aux: Dict = field(default_factory=dict)  # tokens, selected positions, etc.

@dataclass
class Query:
    query_id: str
    text: str
    context: Optional[str] = None
    encoder_id: str = ""
    strategy: str = ""
    q_vecs: Optional[VectorGroup] = None
    coarse_vec: Optional[Vector] = None
    aux: Dict = field(default_factory=dict)

@dataclass
class RetrieveResult:
    mem_id: str
    score: float
    coarse_score: float
    debug: Dict = field(default_factory=dict)

@dataclass
class RouteResult:
    mem_id: str
    weight: float
    selected: bool
    debug: Dict = field(default_factory=dict)

# =========================
# Encoder: pretrained model wrapper
# =========================

class Encoder:
    """
    Encoder: pretrained model wrapper.
    - encode_tokens: returns token vectors (n, d) and token strings
    - encode_sentence: returns a single sentence vector (d,)
    """
    def __init__(self, encoder_id: str):
        self.encoder_id = encoder_id
        # load model here (e.g. bert / sentence embedding model)

    def encode_tokens(self, text: str) -> Tuple[VectorGroup, List[str]]:
        # TODO: implement using transformers
        # return token_vecs, tokens
        raise NotImplementedError

    def encode_sentence(self, text: str) -> Vector:
        # TODO: implement using sentence embedding model
        raise NotImplementedError


# =========================
# Vectorizer: build vector groups (my "field"/"multi-vector" representation)
# =========================

class Vectorizer:
    """
    Strategies:
    - token_pool_topK: select K informative token vectors as the vector group
    - cluster_centers_r: cluster token vectors -> r centers (more 'field-like')
    """
    def __init__(self, strategy: str, k: int = 32, r: int = 6):
        self.strategy = strategy
        self.k = k
        self.r = r

    def make_group(self, token_vecs: VectorGroup, tokens: List[str], text: str) -> Tuple[VectorGroup, Dict]:
        if self.strategy == "token_pool_topK":
            return self._token_pool_topK(token_vecs, tokens, text, k=self.k)
        elif self.strategy == "cluster_centers_r":
            return self._cluster_centers(token_vecs, tokens, text, r=self.r)
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

    def _token_pool_topK(self, token_vecs: VectorGroup, tokens: List[str], text: str, k: int) -> Tuple[VectorGroup, Dict]:
        # MVP heuristic selection:
        # - drop stopwords / punctuation
        # - use TF-IDF or keyword scores to pick topK tokens
        # For pseudocode: pick first K non-trivial tokens as placeholder
        selected_idx = []
        for i, tok in enumerate(tokens):
            if tok.strip() and tok not in {"[CLS]", "[SEP]"}:
                selected_idx.append(i)
            if len(selected_idx) >= k:
                break
        if not selected_idx:
            selected_idx = [0]  # fallback
        group = token_vecs[selected_idx, :]
        aux = {"selected_tokens": [tokens[i] for i in selected_idx], "selected_idx": selected_idx}
        return group, aux

    def _cluster_centers(self, token_vecs: VectorGroup, tokens: List[str], text: str, r: int) -> Tuple[VectorGroup, Dict]:
        # MVP clustering (placeholder):
        # in real impl: k-means on token_vecs after filtering
        # Here: take r evenly spaced token vectors as pseudo-centers
        n = token_vecs.shape[0]
        if n == 0:
            raise ValueError("Empty token vectors")
        idx = np.linspace(0, max(0, n-1), num=min(r, n), dtype=int).tolist()
        group = token_vecs[idx, :]
        aux = {"center_tokens": [tokens[i] for i in idx], "center_idx": idx}
        return group, aux


# =========================
# QueryModel: x -> Q(x)
# MVP: Q is built from encoder output with Vectorizer (no training)
# Trainable version: add a small projection head to transform token vectors before Vectorizer
# =========================

class QueryModel:
    def __init__(self, encoder: Encoder, vectorizer: Vectorizer, projection_head=None):
        self.encoder = encoder
        self.vectorizer = vectorizer
        self.projection_head = projection_head  # small trainable module, optional

    def build_query(self, q: Query) -> Query:
        token_vecs, tokens = self.encoder.encode_tokens(q.text if q.context is None else (q.context + "\n" + q.text))
        if self.projection_head is not None:
            token_vecs = self.projection_head(token_vecs)  # (n, d) -> (n, d) or (n, d2)
        q_vecs, aux = self.vectorizer.make_group(token_vecs, tokens, q.text)
        q.q_vecs = normalize_rows(q_vecs)
        q.aux.update(aux)

        # coarse vector for recall (cheap)
        q.coarse_vec = normalize_vec(self.encoder.encode_sentence(q.text))
        return q


# =========================
# MemoryBuilder: text -> M (vector group) + coarse vec
# =========================

class MemoryBuilder:
    def __init__(self, encoder: Encoder, vectorizer: Vectorizer):
        self.encoder = encoder
        self.vectorizer = vectorizer

    def build_embedding(self, item: MemoryItem, emb_id: str) -> EmbeddingRecord:
        token_vecs, tokens = self.encoder.encode_tokens(item.text)
        m_vecs, aux = self.vectorizer.make_group(token_vecs, tokens, item.text)
        m_vecs = normalize_rows(m_vecs)

        coarse_vec = normalize_vec(self.encoder.encode_sentence(item.text))
        return EmbeddingRecord(
            emb_id=emb_id,
            mem_id=item.mem_id,
            encoder_id=self.encoder.encoder_id,
            strategy=self.vectorizer.strategy,
            dims=int(m_vecs.shape[1]),
            n_vecs=int(m_vecs.shape[0]),
            vecs=m_vecs,
            coarse_vec=coarse_vec,
            aux=aux,
        )


# =========================
# Index: coarse ANN recall (MVP can be brute-force if small)
# =========================

class CoarseIndex:
    def __init__(self):
        self.mem_ids: List[str] = []
        self.coarse_vecs: List[Vector] = []  # store normalized vectors

    def add(self, mem_id: str, coarse_vec: Vector):
        self.mem_ids.append(mem_id)
        self.coarse_vecs.append(coarse_vec)

    def search(self, q_coarse: Vector, top_n: int = 1000) -> List[Tuple[str, float]]:
        # MVP brute-force cosine (= dot since normalized)
        V = np.stack(self.coarse_vecs, axis=0)  # (N, d)
        scores = V @ q_coarse  # (N,)
        idx = np.argsort(-scores)[:min(top_n, len(scores))]
        return [(self.mem_ids[i], float(scores[i])) for i in idx]


# =========================
# Scorer: field-to-field similarity for rerank
# =========================

class FieldScorer:
    def __init__(self, top_k_per_q: int = 3):
        self.top_k_per_q = top_k_per_q

    def score(self, Q: VectorGroup, M: VectorGroup) -> Tuple[float, Dict]:
        # Q: (nq, d), M: (nm, d), both row-normalized
        # sim matrix: (nq, nm)
        S = Q @ M.T
        # per q: best match
        best = S.max(axis=1)  # (nq,)
        # aggregate: top-k mean to reduce accidental spikes
        k = min(self.top_k_per_q, best.shape[0])
        top = np.sort(best)[-k:]
        score = float(top.mean()) if k > 0 else float(best.mean())
        debug = {"best_per_q": best.tolist(), "top_used": top.tolist()}
        return score, debug


# =========================
# Store: keep MemoryItem + EmbeddingRecord mapping
# =========================

class MemoryStore:
    def __init__(self):
        self.items: Dict[str, MemoryItem] = {}
        self.embs: Dict[str, EmbeddingRecord] = {}  # mem_id -> embedding record (choose one strategy per store for MVP)

    def add(self, item: MemoryItem, emb: EmbeddingRecord):
        self.items[item.mem_id] = item
        self.embs[item.mem_id] = emb


# =========================
# Retriever: coarse recall + field rerank
# =========================

class Router:
    """
    Route candidates with soft/half-hard/hard policies.
    - soft: keep all candidates, just produce weights
    - half_hard: only top_k are selected for main path, others get weak weights
    - hard: only top_k are visible
    """
    def __init__(self, policy: str = "soft", top_k: int = 10, weak_weight: float = 0.05):
        self.policy = policy
        self.top_k = top_k
        self.weak_weight = weak_weight

    def route(self, scored: List[Tuple[str, float]]) -> List[RouteResult]:
        # scored: [(mem_id, score)]
        ranked = sorted(scored, key=lambda x: x[1], reverse=True)
        soft_weights = softmax_weights([s for _, s in ranked]) if self.policy == "soft" else []
        results: List[RouteResult] = []
        for i, (mem_id, score) in enumerate(ranked):
            if self.policy == "soft":
                weight = soft_weights[i]
                results.append(RouteResult(mem_id=mem_id, weight=weight, selected=True))
            elif self.policy == "half_hard":
                selected = i < self.top_k
                weight = 1.0 if selected else self.weak_weight
                results.append(RouteResult(mem_id=mem_id, weight=weight, selected=selected))
            elif self.policy == "hard":
                selected = i < self.top_k
                if selected:
                    results.append(RouteResult(mem_id=mem_id, weight=1.0, selected=True))
            else:
                raise ValueError(f"Unknown routing policy: {self.policy}")
        return results


class Retriever:
    def __init__(self, store: MemoryStore, index: CoarseIndex, scorer: FieldScorer, router: Router):
        self.store = store
        self.index = index
        self.scorer = scorer
        self.router = router

    def retrieve(self, q: Query, top_n: int = 1000, top_k: int = 10) -> List[RetrieveResult]:
        assert q.coarse_vec is not None and q.q_vecs is not None, "Query vectors not built"
        candidates = self.index.search(q.coarse_vec, top_n=top_n)

        scored: List[Tuple[str, float]] = []
        for mem_id, coarse_score in candidates:
            emb = self.store.embs[mem_id]
            score, dbg = self.scorer.score(q.q_vecs, emb.vecs)
            scored.append((mem_id, score))

        routed = self.router.route(scored)
        results: List[RetrieveResult] = []
        for route in routed:
            if not route.selected and self.router.policy == "hard":
                continue
            score = next(s for m, s in scored if m == route.mem_id)
            weighted_score = score * route.weight
            results.append(
                RetrieveResult(
                    mem_id=route.mem_id,
                    score=weighted_score,
                    coarse_score=0.0,
                    debug={"route_weight": route.weight, "route_selected": route.selected},
                )
            )

        results.sort(key=lambda r: r.score, reverse=True)
        return results[:top_k]


# =========================
# Training (optional): self-supervised x->Q closer to y->K
# This is where my real goal lives: learn the mapping x->Q(x).
# MVP can skip; later you train projection_head or a small reranker.
# =========================

class PairDataset:
    def __init__(self, pairs: List[Tuple[str, str]]):
        self.pairs = pairs  # (x_text, y_text)

class ProjectionHead:
    """
    Small trainable module.
    For real code, implement with PyTorch:
      token_vecs (n,d) -> token_vecs' (n,d)
    """
    def __call__(self, token_vecs: VectorGroup) -> VectorGroup:
        return token_vecs  # placeholder (identity)

class SelfSupervisedTrainer:
    def __init__(self, query_model: QueryModel, key_model: QueryModel):
        self.query_model = query_model  # x -> Q(x)
        self.key_model = key_model      # y -> K(y) (can share encoder)

    def train_projection_head(self, dataset: PairDataset, epochs: int = 1, batch_size: int = 16):
        """
        Sketch:
        - Build Q(x_i) and K(y_i) as vector groups.
        - Define similarity between groups (e.g. max-sim aggregation).
        - Use in-batch negatives: K(y_j) for j!=i.
        - Optimize projection_head parameters to make sim(Q_i, K_i) > sim(Q_i, K_j).
        """
        # TODO: implement with PyTorch
        pass


# =========================
# Utilities
# =========================

def normalize_vec(v: Vector) -> Vector:
    n = np.linalg.norm(v) + 1e-12
    return v / n

def normalize_rows(V: VectorGroup) -> VectorGroup:
    n = np.linalg.norm(V, axis=1, keepdims=True) + 1e-12
    return V / n

def softmax_weights(scores: List[float]) -> List[float]:
    # small helper for soft routing weights
    vec = np.array(scores)
    exps = np.exp(vec - vec.max())
    probs = exps / (exps.sum() + 1e-12)
    return probs.tolist()
```

## 我应该怎么用这份骨架（对应“十几个小时 2060 预算”）

1. 先实现 `Encoder`：用现成模型把 `encode_tokens` / `encode_sentence` 填上（这是最费事但最核心的）。
2. 先不训练：`projection_head=None`，跑通 `MemoryBuilder -> Index -> Retriever`。
3. 先把参数固定：比如记忆向量组 32 个，query 向量组 32 个，粗召回 300/1000，精排 top-k 10。
4. 跑一个小评测集：20~50 个输入句子，看 top-k 是否“像样”。
5. 再考虑训练：我能接受 2060 跑十几个小时，就可以训练 `ProjectionHead`（小头）或一个更小的 reranker；底座模型不动。

这就是：目标没变（训练 $x → Q$），但我先搭舞台再练功。舞台搭好了，训练时每一小时都能看到“到底变好还是变坏”。

如果我接下来愿意更进一步，我可以按偏好的中文生态挑一两个“最省心的中文 encoder 组合”（句向量 + token 向量），并把 `encode_tokens`/`encode_sentence` 的实现选型写成“安装—调用—输出形状”的清单，避免在库里迷路。
