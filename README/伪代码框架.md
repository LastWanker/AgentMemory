我们的终极目标一直是：训练一个映射

文本 
𝑥
x → 索引向量组/索引场 
𝑄
(
𝑥
)
Q(x)
让它更像“该唤醒什么”的检索指令，而不是“这段话大概讲啥”。

我把“训练 
𝑥
→
𝑄
x→Q”放到 MVP 之外的原因只有一个：先把检索闭环跑通，才能知道训练到底该优化什么。否则你会在没有仪表盘的情况下拧螺丝，拧到天荒地老也不知道方向对不对。

下面给你一份可扩展到训练版的 Python 伪代码骨架：MVP 先用“预训练编码器 + 向量组构造”直接产出 
𝑄
Q；训练版再在 QueryModel 里加一个可训练的小头（projection head / adapter），让 
𝑄
Q 变成“学出来的”。

# =========================
# Data structures
# =========================

from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
import numpy as np
import time

Vector = np.ndarray  # shape: (d,)
VectorGroup = np.ndarray  # shape: (n_vecs, d)

@dataclass
class MemoryItem:
    mem_id: str
    text: str
    created_at: float = field(default_factory=lambda: time.time())
    source: str = "unknown"
    meta: Dict = field(default_factory=dict)

@dataclass
class EmbeddingRecord:
    emb_id: str
    mem_id: str
    encoder_id: str
    strategy: str
    dims: int
    n_vecs: int
    vecs: VectorGroup                   # (n_vecs, d)
    coarse_vec: Optional[Vector] = None # (d,), for coarse recall
    aux: Dict = field(default_factory=dict)  # tokens, selected positions, etc.

@dataclass
class Query:
    query_id: str
    text: str
    context: Optional[str] = None
    encoder_id: str = ""
    strategy: str = ""
    q_vecs: Optional[VectorGroup] = None
    coarse_vec: Optional[Vector] = None
    aux: Dict = field(default_factory=dict)

@dataclass
class RetrieveResult:
    mem_id: str
    score: float
    coarse_score: float
    debug: Dict = field(default_factory=dict)


# =========================
# Encoder: pretrained model wrapper
# =========================

class Encoder:
    """
    Encoder: pretrained model wrapper.
    - encode_tokens: returns token vectors (n, d) and token strings
    - encode_sentence: returns a single sentence vector (d,)
    """
    def __init__(self, encoder_id: str):
        self.encoder_id = encoder_id
        # load model here (e.g. bert / sentence embedding model)

    def encode_tokens(self, text: str) -> Tuple[VectorGroup, List[str]]:
        # TODO: implement using transformers
        # return token_vecs, tokens
        raise NotImplementedError

    def encode_sentence(self, text: str) -> Vector:
        # TODO: implement using sentence embedding model
        raise NotImplementedError


# =========================
# Vectorizer: build vector groups (your "field"/"multi-vector" representation)
# =========================

class Vectorizer:
    """
    Strategies:
    - token_pool_topK: select K informative token vectors as the vector group
    - cluster_centers_r: cluster token vectors -> r centers (more 'field-like')
    """
    def __init__(self, strategy: str, k: int = 32, r: int = 6):
        self.strategy = strategy
        self.k = k
        self.r = r

    def make_group(self, token_vecs: VectorGroup, tokens: List[str], text: str) -> Tuple[VectorGroup, Dict]:
        if self.strategy == "token_pool_topK":
            return self._token_pool_topK(token_vecs, tokens, text, k=self.k)
        elif self.strategy == "cluster_centers_r":
            return self._cluster_centers(token_vecs, tokens, text, r=self.r)
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

    def _token_pool_topK(self, token_vecs: VectorGroup, tokens: List[str], text: str, k: int) -> Tuple[VectorGroup, Dict]:
        # MVP heuristic selection:
        # - drop stopwords / punctuation
        # - use TF-IDF or keyword scores to pick topK tokens
        # For pseudocode: pick first K non-trivial tokens as placeholder
        selected_idx = []
        for i, tok in enumerate(tokens):
            if tok.strip() and tok not in {"[CLS]", "[SEP]"}:
                selected_idx.append(i)
            if len(selected_idx) >= k:
                break
        if not selected_idx:
            selected_idx = [0]  # fallback
        group = token_vecs[selected_idx, :]
        aux = {"selected_tokens": [tokens[i] for i in selected_idx], "selected_idx": selected_idx}
        return group, aux

    def _cluster_centers(self, token_vecs: VectorGroup, tokens: List[str], text: str, r: int) -> Tuple[VectorGroup, Dict]:
        # MVP clustering (placeholder):
        # in real impl: k-means on token_vecs after filtering
        # Here: take r evenly spaced token vectors as pseudo-centers
        n = token_vecs.shape[0]
        if n == 0:
            raise ValueError("Empty token vectors")
        idx = np.linspace(0, max(0, n-1), num=min(r, n), dtype=int).tolist()
        group = token_vecs[idx, :]
        aux = {"center_tokens": [tokens[i] for i in idx], "center_idx": idx}
        return group, aux


# =========================
# QueryModel: x -> Q(x)
# MVP: Q is built from encoder output with Vectorizer (no training)
# Trainable version: add a small projection head to transform token vectors before Vectorizer
# =========================

class QueryModel:
    def __init__(self, encoder: Encoder, vectorizer: Vectorizer, projection_head=None):
        self.encoder = encoder
        self.vectorizer = vectorizer
        self.projection_head = projection_head  # small trainable module, optional

    def build_query(self, q: Query) -> Query:
        token_vecs, tokens = self.encoder.encode_tokens(q.text if q.context is None else (q.context + "\n" + q.text))
        if self.projection_head is not None:
            token_vecs = self.projection_head(token_vecs)  # (n, d) -> (n, d) or (n, d2)
        q_vecs, aux = self.vectorizer.make_group(token_vecs, tokens, q.text)
        q.q_vecs = normalize_rows(q_vecs)
        q.aux.update(aux)

        # coarse vector for recall (cheap)
        q.coarse_vec = normalize_vec(self.encoder.encode_sentence(q.text))
        return q


# =========================
# MemoryBuilder: text -> M (vector group) + coarse vec
# =========================

class MemoryBuilder:
    def __init__(self, encoder: Encoder, vectorizer: Vectorizer):
        self.encoder = encoder
        self.vectorizer = vectorizer

    def build_embedding(self, item: MemoryItem, emb_id: str) -> EmbeddingRecord:
        token_vecs, tokens = self.encoder.encode_tokens(item.text)
        m_vecs, aux = self.vectorizer.make_group(token_vecs, tokens, item.text)
        m_vecs = normalize_rows(m_vecs)

        coarse_vec = normalize_vec(self.encoder.encode_sentence(item.text))
        return EmbeddingRecord(
            emb_id=emb_id,
            mem_id=item.mem_id,
            encoder_id=self.encoder.encoder_id,
            strategy=self.vectorizer.strategy,
            dims=int(m_vecs.shape[1]),
            n_vecs=int(m_vecs.shape[0]),
            vecs=m_vecs,
            coarse_vec=coarse_vec,
            aux=aux,
        )


# =========================
# Index: coarse ANN recall (MVP can be brute-force if small)
# =========================

class CoarseIndex:
    def __init__(self):
        self.mem_ids: List[str] = []
        self.coarse_vecs: List[Vector] = []  # store normalized vectors

    def add(self, mem_id: str, coarse_vec: Vector):
        self.mem_ids.append(mem_id)
        self.coarse_vecs.append(coarse_vec)

    def search(self, q_coarse: Vector, top_n: int = 1000) -> List[Tuple[str, float]]:
        # MVP brute-force cosine (= dot since normalized)
        V = np.stack(self.coarse_vecs, axis=0)  # (N, d)
        scores = V @ q_coarse  # (N,)
        idx = np.argsort(-scores)[:min(top_n, len(scores))]
        return [(self.mem_ids[i], float(scores[i])) for i in idx]


# =========================
# Scorer: field-to-field similarity for rerank
# =========================

class FieldScorer:
    def __init__(self, top_k_per_q: int = 3):
        self.top_k_per_q = top_k_per_q

    def score(self, Q: VectorGroup, M: VectorGroup) -> Tuple[float, Dict]:
        # Q: (nq, d), M: (nm, d), both row-normalized
        # sim matrix: (nq, nm)
        S = Q @ M.T
        # per q: best match
        best = S.max(axis=1)  # (nq,)
        # aggregate: top-k mean to reduce accidental spikes
        k = min(self.top_k_per_q, best.shape[0])
        top = np.sort(best)[-k:]
        score = float(top.mean()) if k > 0 else float(best.mean())
        debug = {"best_per_q": best.tolist(), "top_used": top.tolist()}
        return score, debug


# =========================
# Store: keep MemoryItem + EmbeddingRecord mapping
# =========================

class MemoryStore:
    def __init__(self):
        self.items: Dict[str, MemoryItem] = {}
        self.embs: Dict[str, EmbeddingRecord] = {}  # mem_id -> embedding record (choose one strategy per store for MVP)

    def add(self, item: MemoryItem, emb: EmbeddingRecord):
        self.items[item.mem_id] = item
        self.embs[item.mem_id] = emb


# =========================
# Retriever: coarse recall + field rerank
# =========================

class Retriever:
    def __init__(self, store: MemoryStore, index: CoarseIndex, scorer: FieldScorer):
        self.store = store
        self.index = index
        self.scorer = scorer

    def retrieve(self, q: Query, top_n: int = 1000, top_k: int = 10) -> List[RetrieveResult]:
        assert q.coarse_vec is not None and q.q_vecs is not None, "Query vectors not built"
        candidates = self.index.search(q.coarse_vec, top_n=top_n)

        results: List[RetrieveResult] = []
        for mem_id, coarse_score in candidates:
            emb = self.store.embs[mem_id]
            score, dbg = self.scorer.score(q.q_vecs, emb.vecs)
            results.append(RetrieveResult(mem_id=mem_id, score=score, coarse_score=coarse_score, debug=dbg))

        results.sort(key=lambda r: r.score, reverse=True)
        return results[:top_k]


# =========================
# Training (optional): self-supervised x->Q closer to y->K
# This is where your real goal lives: learn the mapping x->Q(x).
# MVP can skip; later you train projection_head or a small reranker.
# =========================

class PairDataset:
    def __init__(self, pairs: List[Tuple[str, str]]):
        self.pairs = pairs  # (x_text, y_text)

class ProjectionHead:
    """
    Small trainable module.
    For real code, implement with PyTorch:
      token_vecs (n,d) -> token_vecs' (n,d)
    """
    def __call__(self, token_vecs: VectorGroup) -> VectorGroup:
        return token_vecs  # placeholder (identity)

class SelfSupervisedTrainer:
    def __init__(self, query_model: QueryModel, key_model: QueryModel):
        self.query_model = query_model  # x -> Q(x)
        self.key_model = key_model      # y -> K(y) (can share encoder)

    def train_projection_head(self, dataset: PairDataset, epochs: int = 1, batch_size: int = 16):
        """
        Sketch:
        - Build Q(x_i) and K(y_i) as vector groups.
        - Define similarity between groups (e.g. max-sim aggregation).
        - Use in-batch negatives: K(y_j) for j!=i.
        - Optimize projection_head parameters to make sim(Q_i, K_i) > sim(Q_i, K_j).
        """
        # TODO: implement with PyTorch
        pass


# =========================
# Utilities
# =========================

def normalize_vec(v: Vector) -> Vector:
    n = np.linalg.norm(v) + 1e-12
    return v / n

def normalize_rows(V: VectorGroup) -> VectorGroup:
    n = np.linalg.norm(V, axis=1, keepdims=True) + 1e-12
    return V / n

你应该怎么用这份骨架（对应你的“十几个小时2060预算”）

先实现 Encoder：用现成模型把 encode_tokens / encode_sentence 填上（这是最费事但最核心的）

先不训练：projection_head=None，跑通 MemoryBuilder -> Index -> Retriever

先把参数固定：比如记忆向量组 32 个，query 向量组 32 个，粗召回 300/1000，精排 top-k 10

跑一个小评测集：20~50 个输入句子，看 top-k 是否“像样”

再考虑训练：你能接受 2060 跑十几个小时，那完全可以训练 ProjectionHead（小头）或一个更小的 reranker；底座模型不动

这就是：目标没变（训练 
𝑥
→
𝑄
x→Q），但我们先搭舞台再练功。舞台搭好了，你训练时每一小时都能看到“到底变好还是变坏”。

如果你接下来愿意更进一步，我可以按你偏好的中文生态给你挑一两个“最省心的中文 encoder 组合”（句向量 + token 向量），并把 encode_tokens/encode_sentence 的实现选型写成“安装—调用—输出形状”的清单，避免你在库里迷路。